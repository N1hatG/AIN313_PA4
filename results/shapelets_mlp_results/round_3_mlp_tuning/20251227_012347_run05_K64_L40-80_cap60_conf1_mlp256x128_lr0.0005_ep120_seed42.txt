=== Shapelets + MLP (Random Shapelet Transform) ===
 
[Hyperparameters]
{
  "RANDOM_SEED": 42,
  "USE_CONF": true,
  "NUM_SHAPELETS": 64,
  "LEN_MIN": 40,
  "LEN_MAX": 80,
  "MAX_TRAIN_PER_CLASS": 60,
  "MLP_HIDDEN": [
    256,
    128
  ],
  "MLP_MAX_ITER": 120,
  "MLP_LR": 0.0005,
  "test_size": 0.2,
  "val_split": 0.15,
  "batch_size": 128,
  "weight_decay": 0.0001,
  "dropout": 0.2,
  "early_stopping_patience": 12,
  "BEST_VAL_ACC": 0.7407407407407407,
  "BEST_EPOCH": 42,
  "EPOCHS_RUN": 54,
  "CHECKPOINT_PATH": "C:\\Users\\User\\Desktop\\AIN313_PA4\\shapelets_mlp_results\\round_3_mlp_tuning\\20251227_012347_run05_K64_L40-80_cap60_conf1_mlp256x128_lr0.0005_ep120_seed42.pt",
  "MACRO_F1": 0.732475027468609,
  "WEIGHTED_F1": 0.732475027468609,
  "TOP_CONFUSIONS": "walking -> jogging : 7\njogging -> running : 6\nrunning -> jogging : 5\njogging -> walking : 4\nrunning -> walking : 3\nwalking -> running : 2\nhandwaving -> handclapping : 2\nhandclapping -> handwaving : 2\nrunning -> boxing : 1",
  "PER_CLASS_TABLE": "class            prec    rec     f1   supp\n------------------------------------------\nboxing          0.952  1.000  0.976     20\nhandclapping    0.900  0.900  0.900     20\nhandwaving      0.900  0.900  0.900     20\njogging         0.455  0.500  0.476     20\nrunning         0.579  0.550  0.564     20\nwalking         0.611  0.550  0.579     20",
  "TRAINING_CURVES": "epoch    tr_loss   tr_acc    va_loss   va_acc\n----------------------------------------------\n    1     1.7813   0.2190     1.6840   0.5000\n    2     1.6759   0.4248     1.5785   0.5000\n    3     1.5789   0.4477     1.4808   0.4815\n    4     1.4885   0.4804     1.3881   0.4630\n    5     1.4153   0.4706     1.3024   0.4815\n    6     1.3334   0.4510     1.2222   0.4815\n    7     1.2689   0.4902     1.1487   0.5185\n    8     1.2078   0.5327     1.0855   0.5185\n    9     1.1507   0.5229     1.0361   0.6111\n   10     1.1158   0.5686     0.9949   0.6667\n   11     1.0570   0.5850     0.9594   0.6667\n   12     1.0446   0.5588     0.9280   0.6667\n   13     1.0073   0.5654     0.9012   0.6667\n   14     0.9716   0.6078     0.8740   0.6667\n   15     0.9238   0.6111     0.8531   0.5926\n   16     0.8976   0.6242     0.8370   0.6852\n   17     0.8908   0.5980     0.8270   0.6481\n   18     0.8659   0.6471     0.8155   0.6296\n   19     0.8478   0.6144     0.8004   0.6481\n   20     0.8469   0.6013     0.7914   0.6296\n   21     0.8090   0.6536     0.7790   0.6481\n   22     0.7757   0.6536     0.7660   0.6481\n   23     0.7911   0.6569     0.7523   0.6481\n   24     0.7319   0.6863     0.7385   0.6852\n   25     0.7595   0.6569     0.7253   0.6852\n   26     0.7135   0.6961     0.7155   0.6852\n   27     0.7318   0.6634     0.7081   0.7037\n   28     0.7093   0.6895     0.7092   0.6852\n   29     0.7054   0.7092     0.7043   0.6852\n   30     0.7027   0.7157     0.6918   0.7037\n   31     0.7056   0.7059     0.6845   0.7037\n   32     0.6912   0.6895     0.6817   0.7222\n   33     0.6781   0.6797     0.6815   0.7037\n   34     0.6800   0.7124     0.6829   0.7222\n   35     0.6539   0.7288     0.6802   0.7037\n   36     0.6403   0.7418     0.6648   0.6852\n   37     0.6479   0.7222     0.6534   0.6852\n   38     0.6434   0.7059     0.6469   0.7037\n   39     0.6210   0.7288     0.6457   0.7222\n   40     0.6383   0.7190     0.6480   0.7037\n   41     0.6204   0.7386     0.6454   0.7222\n   42     0.6198   0.7386     0.6410   0.7407\n   43     0.6116   0.7222     0.6339   0.7222\n   44     0.6046   0.7386     0.6272   0.7037\n   45     0.5986   0.7778     0.6283   0.6852\n   46     0.6025   0.7549     0.6249   0.6852\n   47     0.5974   0.7484     0.6165   0.7037\n   48     0.5584   0.7843     0.6134   0.7037\n   49     0.5757   0.7320     0.6127   0.7037\n   50     0.5780   0.7778     0.6146   0.7222\n   51     0.5698   0.7647     0.6170   0.7222\n   52     0.5781   0.7614     0.6142   0.7037\n   53     0.5566   0.7614     0.6040   0.7037\n   54     0.5328   0.7876     0.5918   0.7407"
}
 
[Split / Data]
{
  "total_files": 599,
  "train_files_before_cap": 479,
  "test_files": 120,
  "train_sequences_after_cap": 360,
  "class_names": [
    "boxing",
    "handclapping",
    "handwaving",
    "jogging",
    "running",
    "walking"
  ]
}
 
[Timings (seconds)]
{
  "load_data": 0.4624,
  "sample_shapelets": 0.0052,
  "transform_train": 248.7089,
  "transform_test": 80.8897,
  "mlp_fit": 0.5291,
  "eval": 0.006,
  "total": 330.6113
}
 
[Accuracy]
0.733333

[Confusion Matrix]
            BX    HC    HW    JG    RN    WK
BX     |    20     0     0     0     0     0
HC     |     0    18     2     0     0     0
HW     |     0     2    18     0     0     0
JG     |     0     0     0    10     6     4
RN     |     1     0     0     5    11     3
WK     |     0     0     0     7     2    11
 
[Classification Report]
              precision    recall  f1-score   support

      boxing     0.9524    1.0000    0.9756        20
handclapping     0.9000    0.9000    0.9000        20
  handwaving     0.9000    0.9000    0.9000        20
     jogging     0.4545    0.5000    0.4762        20
     running     0.5789    0.5500    0.5641        20
     walking     0.6111    0.5500    0.5789        20

    accuracy                         0.7333       120
   macro avg     0.7328    0.7333    0.7325       120
weighted avg     0.7328    0.7333    0.7325       120

Macro F1: 0.732475027468609
Weighted F1: 0.732475027468609
 
[Top Confusions]
walking -> jogging : 7
jogging -> running : 6
running -> jogging : 5
jogging -> walking : 4
running -> walking : 3
walking -> running : 2
handwaving -> handclapping : 2
handclapping -> handwaving : 2
running -> boxing : 1
 
[Per-class Table]
class            prec    rec     f1   supp
------------------------------------------
boxing          0.952  1.000  0.976     20
handclapping    0.900  0.900  0.900     20
handwaving      0.900  0.900  0.900     20
jogging         0.455  0.500  0.476     20
running         0.579  0.550  0.564     20
walking         0.611  0.550  0.579     20
 
[Training Curves]
epoch    tr_loss   tr_acc    va_loss   va_acc
----------------------------------------------
    1     1.7813   0.2190     1.6840   0.5000
    2     1.6759   0.4248     1.5785   0.5000
    3     1.5789   0.4477     1.4808   0.4815
    4     1.4885   0.4804     1.3881   0.4630
    5     1.4153   0.4706     1.3024   0.4815
    6     1.3334   0.4510     1.2222   0.4815
    7     1.2689   0.4902     1.1487   0.5185
    8     1.2078   0.5327     1.0855   0.5185
    9     1.1507   0.5229     1.0361   0.6111
   10     1.1158   0.5686     0.9949   0.6667
   11     1.0570   0.5850     0.9594   0.6667
   12     1.0446   0.5588     0.9280   0.6667
   13     1.0073   0.5654     0.9012   0.6667
   14     0.9716   0.6078     0.8740   0.6667
   15     0.9238   0.6111     0.8531   0.5926
   16     0.8976   0.6242     0.8370   0.6852
   17     0.8908   0.5980     0.8270   0.6481
   18     0.8659   0.6471     0.8155   0.6296
   19     0.8478   0.6144     0.8004   0.6481
   20     0.8469   0.6013     0.7914   0.6296
   21     0.8090   0.6536     0.7790   0.6481
   22     0.7757   0.6536     0.7660   0.6481
   23     0.7911   0.6569     0.7523   0.6481
   24     0.7319   0.6863     0.7385   0.6852
   25     0.7595   0.6569     0.7253   0.6852
   26     0.7135   0.6961     0.7155   0.6852
   27     0.7318   0.6634     0.7081   0.7037
   28     0.7093   0.6895     0.7092   0.6852
   29     0.7054   0.7092     0.7043   0.6852
   30     0.7027   0.7157     0.6918   0.7037
   31     0.7056   0.7059     0.6845   0.7037
   32     0.6912   0.6895     0.6817   0.7222
   33     0.6781   0.6797     0.6815   0.7037
   34     0.6800   0.7124     0.6829   0.7222
   35     0.6539   0.7288     0.6802   0.7037
   36     0.6403   0.7418     0.6648   0.6852
   37     0.6479   0.7222     0.6534   0.6852
   38     0.6434   0.7059     0.6469   0.7037
   39     0.6210   0.7288     0.6457   0.7222
   40     0.6383   0.7190     0.6480   0.7037
   41     0.6204   0.7386     0.6454   0.7222
   42     0.6198   0.7386     0.6410   0.7407
   43     0.6116   0.7222     0.6339   0.7222
   44     0.6046   0.7386     0.6272   0.7037
   45     0.5986   0.7778     0.6283   0.6852
   46     0.6025   0.7549     0.6249   0.6852
   47     0.5974   0.7484     0.6165   0.7037
   48     0.5584   0.7843     0.6134   0.7037
   49     0.5757   0.7320     0.6127   0.7037
   50     0.5780   0.7778     0.6146   0.7222
   51     0.5698   0.7647     0.6170   0.7222
   52     0.5781   0.7614     0.6142   0.7037
   53     0.5566   0.7614     0.6040   0.7037
   54     0.5328   0.7876     0.5918   0.7407
 
