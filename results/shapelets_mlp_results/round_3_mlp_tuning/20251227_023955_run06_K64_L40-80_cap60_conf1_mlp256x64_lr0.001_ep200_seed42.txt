=== Shapelets + MLP (Random Shapelet Transform) ===
 
[Hyperparameters]
{
  "RANDOM_SEED": 42,
  "USE_CONF": true,
  "NUM_SHAPELETS": 64,
  "LEN_MIN": 40,
  "LEN_MAX": 80,
  "MAX_TRAIN_PER_CLASS": 60,
  "MLP_HIDDEN": [
    256,
    64
  ],
  "MLP_MAX_ITER": 200,
  "MLP_LR": 0.001,
  "test_size": 0.2,
  "val_split": 0.15,
  "batch_size": 128,
  "weight_decay": 0.0001,
  "dropout": 0.2,
  "early_stopping_patience": 12,
  "BEST_VAL_ACC": 0.8333333333333334,
  "BEST_EPOCH": 42,
  "EPOCHS_RUN": 54,
  "CHECKPOINT_PATH": "C:\\Users\\User\\Desktop\\AIN313_PA4\\shapelets_mlp_results\\round_3_mlp_tuning\\20251227_023955_run06_K64_L40-80_cap60_conf1_mlp256x64_lr0.001_ep200_seed42.pt",
  "MACRO_F1": 0.6942369136371584,
  "WEIGHTED_F1": 0.6942369136371584,
  "TOP_CONFUSIONS": "running -> jogging : 9\njogging -> walking : 6\nrunning -> walking : 5\nwalking -> running : 4\nhandwaving -> handclapping : 4\nwalking -> jogging : 3\njogging -> running : 3\nhandclapping -> handwaving : 2",
  "PER_CLASS_TABLE": "class            prec    rec     f1   supp\n------------------------------------------\nboxing          1.000  1.000  1.000     20\nhandclapping    0.818  0.900  0.857     20\nhandwaving      0.889  0.800  0.842     20\njogging         0.478  0.550  0.512     20\nrunning         0.462  0.300  0.364     20\nwalking         0.542  0.650  0.591     20",
  "TRAINING_CURVES": "epoch    tr_loss   tr_acc    va_loss   va_acc\n----------------------------------------------\n    1     1.7832   0.2059     1.6836   0.3148\n    2     1.6396   0.3431     1.5677   0.3704\n    3     1.5130   0.3987     1.4513   0.5556\n    4     1.3794   0.4837     1.3340   0.6667\n    5     1.2855   0.5065     1.2293   0.6667\n    6     1.1741   0.5327     1.1463   0.6667\n    7     1.1115   0.4935     1.0787   0.6667\n    8     1.0536   0.5294     1.0162   0.6667\n    9     1.0029   0.5458     0.9528   0.6667\n   10     0.9769   0.5621     0.8957   0.6481\n   11     0.9094   0.5948     0.8545   0.6296\n   12     0.9034   0.5948     0.8120   0.6481\n   13     0.8726   0.6078     0.7761   0.6852\n   14     0.8222   0.6307     0.7517   0.7037\n   15     0.7999   0.6438     0.7305   0.7037\n   16     0.7920   0.6503     0.7098   0.7037\n   17     0.7771   0.6340     0.6895   0.7037\n   18     0.7575   0.6307     0.6714   0.7222\n   19     0.7567   0.6732     0.6545   0.7222\n   20     0.7268   0.6536     0.6467   0.7037\n   21     0.7072   0.6895     0.6466   0.7407\n   22     0.7084   0.6797     0.6448   0.7222\n   23     0.6944   0.6895     0.6243   0.7037\n   24     0.6706   0.6895     0.6166   0.7407\n   25     0.6782   0.7059     0.6118   0.7222\n   26     0.6575   0.6895     0.6185   0.7037\n   27     0.6519   0.6961     0.6099   0.7037\n   28     0.6383   0.7353     0.5902   0.7407\n   29     0.6333   0.6993     0.5861   0.7593\n   30     0.6279   0.7092     0.5843   0.7407\n   31     0.5797   0.7516     0.5932   0.7222\n   32     0.5856   0.7451     0.5819   0.7407\n   33     0.5754   0.7582     0.5666   0.7778\n   34     0.5869   0.7549     0.5603   0.7407\n   35     0.5634   0.7451     0.5647   0.7593\n   36     0.5546   0.7582     0.5686   0.7593\n   37     0.5465   0.7647     0.5402   0.7407\n   38     0.5355   0.7974     0.5347   0.7593\n   39     0.5259   0.7712     0.5309   0.7778\n   40     0.5287   0.7549     0.5302   0.7778\n   41     0.5093   0.8072     0.5238   0.8148\n   42     0.4872   0.8235     0.5122   0.8333\n   43     0.4634   0.8072     0.5194   0.7963\n   44     0.5020   0.7908     0.5061   0.7778\n   45     0.4598   0.7908     0.5020   0.7407\n   46     0.4632   0.7941     0.5107   0.7407\n   47     0.4769   0.8007     0.5111   0.7778\n   48     0.4650   0.8072     0.5033   0.7778\n   49     0.4492   0.8203     0.4958   0.8333\n   50     0.4478   0.8399     0.4923   0.8333\n   51     0.4379   0.8268     0.4932   0.7778\n   52     0.4232   0.8431     0.5085   0.7778\n   53     0.4192   0.8431     0.4969   0.7963\n   54     0.4248   0.8497     0.4840   0.8148"
}
 
[Split / Data]
{
  "total_files": 599,
  "train_files_before_cap": 479,
  "test_files": 120,
  "train_sequences_after_cap": 360,
  "class_names": [
    "boxing",
    "handclapping",
    "handwaving",
    "jogging",
    "running",
    "walking"
  ]
}
 
[Timings (seconds)]
{
  "load_data": 0.6746,
  "sample_shapelets": 0.003,
  "transform_train": 302.166,
  "transform_test": 101.6494,
  "mlp_fit": 0.535,
  "eval": 0.0061,
  "total": 405.0382
}
 
[Accuracy]
0.700000

[Confusion Matrix]
            BX    HC    HW    JG    RN    WK
BX     |    20     0     0     0     0     0
HC     |     0    18     2     0     0     0
HW     |     0     4    16     0     0     0
JG     |     0     0     0    11     3     6
RN     |     0     0     0     9     6     5
WK     |     0     0     0     3     4    13
 
[Classification Report]
              precision    recall  f1-score   support

      boxing     1.0000    1.0000    1.0000        20
handclapping     0.8182    0.9000    0.8571        20
  handwaving     0.8889    0.8000    0.8421        20
     jogging     0.4783    0.5500    0.5116        20
     running     0.4615    0.3000    0.3636        20
     walking     0.5417    0.6500    0.5909        20

    accuracy                         0.7000       120
   macro avg     0.6981    0.7000    0.6942       120
weighted avg     0.6981    0.7000    0.6942       120

Macro F1: 0.6942369136371584
Weighted F1: 0.6942369136371584
 
[Top Confusions]
running -> jogging : 9
jogging -> walking : 6
running -> walking : 5
walking -> running : 4
handwaving -> handclapping : 4
walking -> jogging : 3
jogging -> running : 3
handclapping -> handwaving : 2
 
[Per-class Table]
class            prec    rec     f1   supp
------------------------------------------
boxing          1.000  1.000  1.000     20
handclapping    0.818  0.900  0.857     20
handwaving      0.889  0.800  0.842     20
jogging         0.478  0.550  0.512     20
running         0.462  0.300  0.364     20
walking         0.542  0.650  0.591     20
 
[Training Curves]
epoch    tr_loss   tr_acc    va_loss   va_acc
----------------------------------------------
    1     1.7832   0.2059     1.6836   0.3148
    2     1.6396   0.3431     1.5677   0.3704
    3     1.5130   0.3987     1.4513   0.5556
    4     1.3794   0.4837     1.3340   0.6667
    5     1.2855   0.5065     1.2293   0.6667
    6     1.1741   0.5327     1.1463   0.6667
    7     1.1115   0.4935     1.0787   0.6667
    8     1.0536   0.5294     1.0162   0.6667
    9     1.0029   0.5458     0.9528   0.6667
   10     0.9769   0.5621     0.8957   0.6481
   11     0.9094   0.5948     0.8545   0.6296
   12     0.9034   0.5948     0.8120   0.6481
   13     0.8726   0.6078     0.7761   0.6852
   14     0.8222   0.6307     0.7517   0.7037
   15     0.7999   0.6438     0.7305   0.7037
   16     0.7920   0.6503     0.7098   0.7037
   17     0.7771   0.6340     0.6895   0.7037
   18     0.7575   0.6307     0.6714   0.7222
   19     0.7567   0.6732     0.6545   0.7222
   20     0.7268   0.6536     0.6467   0.7037
   21     0.7072   0.6895     0.6466   0.7407
   22     0.7084   0.6797     0.6448   0.7222
   23     0.6944   0.6895     0.6243   0.7037
   24     0.6706   0.6895     0.6166   0.7407
   25     0.6782   0.7059     0.6118   0.7222
   26     0.6575   0.6895     0.6185   0.7037
   27     0.6519   0.6961     0.6099   0.7037
   28     0.6383   0.7353     0.5902   0.7407
   29     0.6333   0.6993     0.5861   0.7593
   30     0.6279   0.7092     0.5843   0.7407
   31     0.5797   0.7516     0.5932   0.7222
   32     0.5856   0.7451     0.5819   0.7407
   33     0.5754   0.7582     0.5666   0.7778
   34     0.5869   0.7549     0.5603   0.7407
   35     0.5634   0.7451     0.5647   0.7593
   36     0.5546   0.7582     0.5686   0.7593
   37     0.5465   0.7647     0.5402   0.7407
   38     0.5355   0.7974     0.5347   0.7593
   39     0.5259   0.7712     0.5309   0.7778
   40     0.5287   0.7549     0.5302   0.7778
   41     0.5093   0.8072     0.5238   0.8148
   42     0.4872   0.8235     0.5122   0.8333
   43     0.4634   0.8072     0.5194   0.7963
   44     0.5020   0.7908     0.5061   0.7778
   45     0.4598   0.7908     0.5020   0.7407
   46     0.4632   0.7941     0.5107   0.7407
   47     0.4769   0.8007     0.5111   0.7778
   48     0.4650   0.8072     0.5033   0.7778
   49     0.4492   0.8203     0.4958   0.8333
   50     0.4478   0.8399     0.4923   0.8333
   51     0.4379   0.8268     0.4932   0.7778
   52     0.4232   0.8431     0.5085   0.7778
   53     0.4192   0.8431     0.4969   0.7963
   54     0.4248   0.8497     0.4840   0.8148
 
