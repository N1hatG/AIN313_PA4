=== Shapelets + MLP (Random Shapelet Transform) ===
 
[Hyperparameters]
{
  "RANDOM_SEED": 42,
  "USE_CONF": true,
  "NUM_SHAPELETS": 64,
  "LEN_MIN": 40,
  "LEN_MAX": 80,
  "MAX_TRAIN_PER_CLASS": 60,
  "MLP_HIDDEN": [
    256,
    128
  ],
  "MLP_MAX_ITER": 200,
  "MLP_LR": 0.00095,
  "test_size": 0.2,
  "val_split": 0.15,
  "batch_size": 128,
  "weight_decay": 0.0001,
  "dropout": 0.2,
  "early_stopping_patience": 12,
  "BEST_VAL_ACC": 0.8148148148148148,
  "BEST_EPOCH": 47,
  "EPOCHS_RUN": 59,
  "CHECKPOINT_PATH": "C:\\Users\\User\\Desktop\\AIN313_PA4\\shapelets_mlp_results\\round_3_mlp_tuning\\20251227_150622_run03_K64_L40-80_cap60_conf1_mlp256x128_lr0.00095_ep200_seed42.pt",
  "MACRO_F1": 0.6807112941259282,
  "WEIGHTED_F1": 0.6807112941259281,
  "TOP_CONFUSIONS": "running -> jogging : 7\njogging -> walking : 7\nhandwaving -> handclapping : 6\nrunning -> walking : 5\njogging -> running : 5\nwalking -> running : 4\nhandclapping -> handwaving : 2\nrunning -> boxing : 1",
  "PER_CLASS_TABLE": "class            prec    rec     f1   supp\n------------------------------------------\nboxing          0.952  1.000  0.976     20\nhandclapping    0.750  0.900  0.818     20\nhandwaving      0.875  0.700  0.778     20\njogging         0.533  0.400  0.457     20\nrunning         0.438  0.350  0.389     20\nwalking         0.571  0.800  0.667     20",
  "TRAINING_CURVES": "epoch    tr_loss   tr_acc    va_loss   va_acc\n----------------------------------------------\n    1     1.7520   0.1993     1.6154   0.3519\n    2     1.5583   0.4216     1.4697   0.3519\n    3     1.4092   0.4150     1.3365   0.4630\n    4     1.2674   0.4641     1.2181   0.5000\n    5     1.1628   0.5654     1.1187   0.6296\n    6     1.0873   0.5719     1.0374   0.6296\n    7     1.0138   0.5817     0.9742   0.6667\n    8     0.9553   0.5980     0.9235   0.6852\n    9     0.9364   0.5817     0.8558   0.7037\n   10     0.8935   0.5980     0.7879   0.6852\n   11     0.8160   0.6601     0.7475   0.6667\n   12     0.8318   0.6144     0.7190   0.7037\n   13     0.7579   0.6732     0.7188   0.7222\n   14     0.7931   0.6438     0.6874   0.6852\n   15     0.7536   0.6503     0.6627   0.6852\n   16     0.7154   0.7124     0.6375   0.6852\n   17     0.7250   0.6765     0.6511   0.6667\n   18     0.6862   0.7092     0.6512   0.6667\n   19     0.6916   0.7026     0.6254   0.7593\n   20     0.6822   0.6830     0.6134   0.7407\n   21     0.6533   0.7026     0.5934   0.7037\n   22     0.6319   0.7484     0.5909   0.6852\n   23     0.6452   0.7288     0.5844   0.7037\n   24     0.6080   0.7484     0.5820   0.7407\n   25     0.6022   0.7549     0.5745   0.7222\n   26     0.5963   0.7745     0.5705   0.6852\n   27     0.5894   0.7451     0.5621   0.7037\n   28     0.5810   0.7712     0.5674   0.7407\n   29     0.5615   0.7647     0.5612   0.7593\n   30     0.5473   0.7647     0.5552   0.7222\n   31     0.5532   0.7778     0.5546   0.7778\n   32     0.5259   0.7908     0.5577   0.7778\n   33     0.5229   0.7941     0.5395   0.7407\n   34     0.5032   0.8072     0.5363   0.7407\n   35     0.5249   0.7908     0.5447   0.7222\n   36     0.4904   0.8007     0.5418   0.7593\n   37     0.4524   0.8464     0.5324   0.7778\n   38     0.4641   0.8235     0.5227   0.7593\n   39     0.4647   0.8072     0.5253   0.7778\n   40     0.4574   0.8170     0.5207   0.7963\n   41     0.4538   0.8366     0.5300   0.7778\n   42     0.4484   0.8333     0.5209   0.7963\n   43     0.4353   0.8203     0.5112   0.7778\n   44     0.4067   0.8562     0.5105   0.7778\n   45     0.4376   0.8333     0.5194   0.7778\n   46     0.4116   0.8235     0.5213   0.7963\n   47     0.4003   0.8562     0.5170   0.8148\n   48     0.3939   0.8497     0.5202   0.7778\n   49     0.4001   0.8562     0.5202   0.7407\n   50     0.3891   0.8301     0.5056   0.7407\n   51     0.3844   0.8333     0.5088   0.7407\n   52     0.3633   0.8693     0.5367   0.7407\n   53     0.4034   0.8235     0.5342   0.7407\n   54     0.3653   0.8562     0.5225   0.7407\n   55     0.3843   0.8497     0.5241   0.7593\n   56     0.3896   0.8399     0.5193   0.7963\n   57     0.3599   0.8693     0.5166   0.7963\n   58     0.3445   0.8791     0.5112   0.7593\n   59     0.3667   0.8595     0.5132   0.7593"
}
 
[Split / Data]
{
  "total_files": 599,
  "train_files_before_cap": 479,
  "test_files": 120,
  "train_sequences_after_cap": 360,
  "class_names": [
    "boxing",
    "handclapping",
    "handwaving",
    "jogging",
    "running",
    "walking"
  ]
}
 
[Timings (seconds)]
{
  "load_data": 14.5824,
  "sample_shapelets": 0.0,
  "transform_train": 326.13,
  "transform_test": 105.3092,
  "mlp_fit": 0.6644,
  "eval": 0.0066,
  "total": 446.6987
}
 
[Accuracy]
0.691667

[Confusion Matrix]
            BX    HC    HW    JG    RN    WK
BX     |    20     0     0     0     0     0
HC     |     0    18     2     0     0     0
HW     |     0     6    14     0     0     0
JG     |     0     0     0     8     5     7
RN     |     1     0     0     7     7     5
WK     |     0     0     0     0     4    16
 
[Classification Report]
              precision    recall  f1-score   support

      boxing     0.9524    1.0000    0.9756        20
handclapping     0.7500    0.9000    0.8182        20
  handwaving     0.8750    0.7000    0.7778        20
     jogging     0.5333    0.4000    0.4571        20
     running     0.4375    0.3500    0.3889        20
     walking     0.5714    0.8000    0.6667        20

    accuracy                         0.6917       120
   macro avg     0.6866    0.6917    0.6807       120
weighted avg     0.6866    0.6917    0.6807       120

Macro F1: 0.6807112941259282
Weighted F1: 0.6807112941259281
 
[Top Confusions]
running -> jogging : 7
jogging -> walking : 7
handwaving -> handclapping : 6
running -> walking : 5
jogging -> running : 5
walking -> running : 4
handclapping -> handwaving : 2
running -> boxing : 1
 
[Per-class Table]
class            prec    rec     f1   supp
------------------------------------------
boxing          0.952  1.000  0.976     20
handclapping    0.750  0.900  0.818     20
handwaving      0.875  0.700  0.778     20
jogging         0.533  0.400  0.457     20
running         0.438  0.350  0.389     20
walking         0.571  0.800  0.667     20
 
[Training Curves]
epoch    tr_loss   tr_acc    va_loss   va_acc
----------------------------------------------
    1     1.7520   0.1993     1.6154   0.3519
    2     1.5583   0.4216     1.4697   0.3519
    3     1.4092   0.4150     1.3365   0.4630
    4     1.2674   0.4641     1.2181   0.5000
    5     1.1628   0.5654     1.1187   0.6296
    6     1.0873   0.5719     1.0374   0.6296
    7     1.0138   0.5817     0.9742   0.6667
    8     0.9553   0.5980     0.9235   0.6852
    9     0.9364   0.5817     0.8558   0.7037
   10     0.8935   0.5980     0.7879   0.6852
   11     0.8160   0.6601     0.7475   0.6667
   12     0.8318   0.6144     0.7190   0.7037
   13     0.7579   0.6732     0.7188   0.7222
   14     0.7931   0.6438     0.6874   0.6852
   15     0.7536   0.6503     0.6627   0.6852
   16     0.7154   0.7124     0.6375   0.6852
   17     0.7250   0.6765     0.6511   0.6667
   18     0.6862   0.7092     0.6512   0.6667
   19     0.6916   0.7026     0.6254   0.7593
   20     0.6822   0.6830     0.6134   0.7407
   21     0.6533   0.7026     0.5934   0.7037
   22     0.6319   0.7484     0.5909   0.6852
   23     0.6452   0.7288     0.5844   0.7037
   24     0.6080   0.7484     0.5820   0.7407
   25     0.6022   0.7549     0.5745   0.7222
   26     0.5963   0.7745     0.5705   0.6852
   27     0.5894   0.7451     0.5621   0.7037
   28     0.5810   0.7712     0.5674   0.7407
   29     0.5615   0.7647     0.5612   0.7593
   30     0.5473   0.7647     0.5552   0.7222
   31     0.5532   0.7778     0.5546   0.7778
   32     0.5259   0.7908     0.5577   0.7778
   33     0.5229   0.7941     0.5395   0.7407
   34     0.5032   0.8072     0.5363   0.7407
   35     0.5249   0.7908     0.5447   0.7222
   36     0.4904   0.8007     0.5418   0.7593
   37     0.4524   0.8464     0.5324   0.7778
   38     0.4641   0.8235     0.5227   0.7593
   39     0.4647   0.8072     0.5253   0.7778
   40     0.4574   0.8170     0.5207   0.7963
   41     0.4538   0.8366     0.5300   0.7778
   42     0.4484   0.8333     0.5209   0.7963
   43     0.4353   0.8203     0.5112   0.7778
   44     0.4067   0.8562     0.5105   0.7778
   45     0.4376   0.8333     0.5194   0.7778
   46     0.4116   0.8235     0.5213   0.7963
   47     0.4003   0.8562     0.5170   0.8148
   48     0.3939   0.8497     0.5202   0.7778
   49     0.4001   0.8562     0.5202   0.7407
   50     0.3891   0.8301     0.5056   0.7407
   51     0.3844   0.8333     0.5088   0.7407
   52     0.3633   0.8693     0.5367   0.7407
   53     0.4034   0.8235     0.5342   0.7407
   54     0.3653   0.8562     0.5225   0.7407
   55     0.3843   0.8497     0.5241   0.7593
   56     0.3896   0.8399     0.5193   0.7963
   57     0.3599   0.8693     0.5166   0.7963
   58     0.3445   0.8791     0.5112   0.7593
   59     0.3667   0.8595     0.5132   0.7593
 
