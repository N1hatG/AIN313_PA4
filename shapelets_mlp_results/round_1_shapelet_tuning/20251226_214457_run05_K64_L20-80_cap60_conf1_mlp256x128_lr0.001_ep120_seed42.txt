=== Shapelets + MLP (Random Shapelet Transform) ===
 
[Hyperparameters]
{
  "RANDOM_SEED": 42,
  "USE_CONF": true,
  "NUM_SHAPELETS": 64,
  "LEN_MIN": 20,
  "LEN_MAX": 80,
  "MAX_TRAIN_PER_CLASS": 60,
  "MLP_HIDDEN": [
    256,
    128
  ],
  "MLP_MAX_ITER": 120,
  "MLP_LR": 0.001,
  "test_size": 0.2,
  "val_split": 0.15,
  "batch_size": 128,
  "weight_decay": 0.0001,
  "dropout": 0.2,
  "early_stopping_patience": 12,
  "BEST_VAL_ACC": 0.8518518518518519,
  "BEST_EPOCH": 38,
  "EPOCHS_RUN": 50,
  "CHECKPOINT_PATH": "C:\\Users\\User\\Desktop\\AIN313_PA4\\shapelets_mlp_results\\20251226_214457_run05_K64_L20-80_cap60_conf1_mlp256x128_lr0.001_ep120_seed42.pt",
  "MACRO_F1": 0.789805619217384,
  "WEIGHTED_F1": 0.7898056192173839,
  "TOP_CONFUSIONS": "walking -> jogging : 7\njogging -> running : 5\nhandwaving -> handclapping : 3\nhandclapping -> handwaving : 3\nwalking -> running : 2\nrunning -> jogging : 2\njogging -> walking : 2\nrunning -> walking : 1",
  "PER_CLASS_TABLE": "class            prec    rec     f1   supp\n------------------------------------------\nboxing          1.000  1.000  1.000     20\nhandclapping    0.850  0.850  0.850     20\nhandwaving      0.850  0.850  0.850     20\njogging         0.591  0.650  0.619     20\nrunning         0.708  0.850  0.773     20\nwalking         0.786  0.550  0.647     20",
  "TRAINING_CURVES": "epoch    tr_loss   tr_acc    va_loss   va_acc\n----------------------------------------------\n    1     1.7532   0.2288     1.5921   0.5185\n    2     1.5822   0.4575     1.4208   0.6481\n    3     1.4293   0.5327     1.2498   0.6667\n    4     1.2756   0.5261     1.0957   0.6852\n    5     1.1554   0.5752     0.9736   0.7037\n    6     1.0652   0.5686     0.8804   0.6852\n    7     1.0020   0.5621     0.8102   0.7222\n    8     0.9292   0.6176     0.7526   0.7407\n    9     0.8747   0.5817     0.7110   0.7407\n   10     0.8201   0.6242     0.6818   0.7222\n   11     0.7957   0.6405     0.6620   0.7407\n   12     0.7599   0.6732     0.6326   0.7407\n   13     0.7094   0.7124     0.6102   0.7593\n   14     0.6981   0.6830     0.5977   0.7407\n   15     0.6493   0.7190     0.5828   0.7407\n   16     0.6337   0.7190     0.5754   0.7037\n   17     0.6250   0.7320     0.5665   0.7037\n   18     0.6031   0.7484     0.5683   0.7222\n   19     0.6098   0.7353     0.5543   0.7407\n   20     0.5993   0.7418     0.5489   0.7593\n   21     0.5676   0.7549     0.5512   0.7593\n   22     0.5552   0.7484     0.5389   0.7407\n   23     0.5267   0.7712     0.5350   0.7593\n   24     0.5127   0.7614     0.5382   0.7778\n   25     0.5309   0.7614     0.5348   0.7407\n   26     0.4747   0.8301     0.5245   0.7963\n   27     0.4736   0.8072     0.5183   0.7778\n   28     0.4910   0.7941     0.5107   0.7778\n   29     0.4787   0.8072     0.5152   0.8148\n   30     0.4677   0.8072     0.5222   0.7593\n   31     0.4402   0.8366     0.5093   0.7593\n   32     0.4503   0.8170     0.5119   0.7407\n   33     0.4412   0.8007     0.5130   0.7778\n   34     0.4237   0.8235     0.5295   0.8148\n   35     0.4303   0.7941     0.5211   0.7963\n   36     0.4114   0.8366     0.5099   0.7407\n   37     0.3959   0.8627     0.5097   0.7593\n   38     0.4148   0.8333     0.5115   0.8519\n   39     0.3860   0.8366     0.5208   0.7963\n   40     0.3842   0.8301     0.5186   0.7593\n   41     0.3706   0.8562     0.5035   0.7593\n   42     0.3803   0.8203     0.5046   0.8148\n   43     0.3772   0.8268     0.5117   0.7778\n   44     0.3539   0.8497     0.5249   0.7778\n   45     0.3428   0.8529     0.5217   0.7407\n   46     0.3338   0.8660     0.5123   0.7778\n   47     0.3310   0.8660     0.5202   0.7778\n   48     0.3319   0.8725     0.5389   0.8148\n   49     0.3175   0.8922     0.5360   0.7407\n   50     0.3187   0.8922     0.5432   0.7593"
}
 
[Split / Data]
{
  "total_files": 599,
  "train_files_before_cap": 479,
  "test_files": 120,
  "train_sequences_after_cap": 360,
  "class_names": [
    "boxing",
    "handclapping",
    "handwaving",
    "jogging",
    "running",
    "walking"
  ]
}
 
[Timings (seconds)]
{
  "load_data": 0.4643,
  "sample_shapelets": 0.0064,
  "transform_train": 218.379,
  "transform_test": 70.8233,
  "mlp_fit": 0.4558,
  "eval": 0.0,
  "total": 290.1289
}
 
[Accuracy]
0.791667

[Confusion Matrix]
            BX    HC    HW    JG    RN    WK
BX     |    20     0     0     0     0     0
HC     |     0    17     3     0     0     0
HW     |     0     3    17     0     0     0
JG     |     0     0     0    13     5     2
RN     |     0     0     0     2    17     1
WK     |     0     0     0     7     2    11
 
[Classification Report]
              precision    recall  f1-score   support

      boxing     1.0000    1.0000    1.0000        20
handclapping     0.8500    0.8500    0.8500        20
  handwaving     0.8500    0.8500    0.8500        20
     jogging     0.5909    0.6500    0.6190        20
     running     0.7083    0.8500    0.7727        20
     walking     0.7857    0.5500    0.6471        20

    accuracy                         0.7917       120
   macro avg     0.7975    0.7917    0.7898       120
weighted avg     0.7975    0.7917    0.7898       120

Macro F1: 0.789805619217384
Weighted F1: 0.7898056192173839
 
[Top Confusions]
walking -> jogging : 7
jogging -> running : 5
handwaving -> handclapping : 3
handclapping -> handwaving : 3
walking -> running : 2
running -> jogging : 2
jogging -> walking : 2
running -> walking : 1
 
[Per-class Table]
class            prec    rec     f1   supp
------------------------------------------
boxing          1.000  1.000  1.000     20
handclapping    0.850  0.850  0.850     20
handwaving      0.850  0.850  0.850     20
jogging         0.591  0.650  0.619     20
running         0.708  0.850  0.773     20
walking         0.786  0.550  0.647     20
 
[Training Curves]
epoch    tr_loss   tr_acc    va_loss   va_acc
----------------------------------------------
    1     1.7532   0.2288     1.5921   0.5185
    2     1.5822   0.4575     1.4208   0.6481
    3     1.4293   0.5327     1.2498   0.6667
    4     1.2756   0.5261     1.0957   0.6852
    5     1.1554   0.5752     0.9736   0.7037
    6     1.0652   0.5686     0.8804   0.6852
    7     1.0020   0.5621     0.8102   0.7222
    8     0.9292   0.6176     0.7526   0.7407
    9     0.8747   0.5817     0.7110   0.7407
   10     0.8201   0.6242     0.6818   0.7222
   11     0.7957   0.6405     0.6620   0.7407
   12     0.7599   0.6732     0.6326   0.7407
   13     0.7094   0.7124     0.6102   0.7593
   14     0.6981   0.6830     0.5977   0.7407
   15     0.6493   0.7190     0.5828   0.7407
   16     0.6337   0.7190     0.5754   0.7037
   17     0.6250   0.7320     0.5665   0.7037
   18     0.6031   0.7484     0.5683   0.7222
   19     0.6098   0.7353     0.5543   0.7407
   20     0.5993   0.7418     0.5489   0.7593
   21     0.5676   0.7549     0.5512   0.7593
   22     0.5552   0.7484     0.5389   0.7407
   23     0.5267   0.7712     0.5350   0.7593
   24     0.5127   0.7614     0.5382   0.7778
   25     0.5309   0.7614     0.5348   0.7407
   26     0.4747   0.8301     0.5245   0.7963
   27     0.4736   0.8072     0.5183   0.7778
   28     0.4910   0.7941     0.5107   0.7778
   29     0.4787   0.8072     0.5152   0.8148
   30     0.4677   0.8072     0.5222   0.7593
   31     0.4402   0.8366     0.5093   0.7593
   32     0.4503   0.8170     0.5119   0.7407
   33     0.4412   0.8007     0.5130   0.7778
   34     0.4237   0.8235     0.5295   0.8148
   35     0.4303   0.7941     0.5211   0.7963
   36     0.4114   0.8366     0.5099   0.7407
   37     0.3959   0.8627     0.5097   0.7593
   38     0.4148   0.8333     0.5115   0.8519
   39     0.3860   0.8366     0.5208   0.7963
   40     0.3842   0.8301     0.5186   0.7593
   41     0.3706   0.8562     0.5035   0.7593
   42     0.3803   0.8203     0.5046   0.8148
   43     0.3772   0.8268     0.5117   0.7778
   44     0.3539   0.8497     0.5249   0.7778
   45     0.3428   0.8529     0.5217   0.7407
   46     0.3338   0.8660     0.5123   0.7778
   47     0.3310   0.8660     0.5202   0.7778
   48     0.3319   0.8725     0.5389   0.8148
   49     0.3175   0.8922     0.5360   0.7407
   50     0.3187   0.8922     0.5432   0.7593
 
