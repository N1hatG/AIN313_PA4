=== Shapelets + MLP (Random Shapelet Transform) ===
 
[Hyperparameters]
{
  "RANDOM_SEED": 42,
  "USE_CONF": true,
  "NUM_SHAPELETS": 64,
  "LEN_MIN": 40,
  "LEN_MAX": 80,
  "MAX_TRAIN_PER_CLASS": 60,
  "MLP_HIDDEN": [
    256,
    128
  ],
  "MLP_MAX_ITER": 200,
  "MLP_LR": 0.001,
  "test_size": 0.2,
  "val_split": 0.15,
  "batch_size": 128,
  "weight_decay": 0.0001,
  "dropout": 0.2,
  "early_stopping_patience": 12,
  "BEST_VAL_ACC": 0.7962962962962963,
  "BEST_EPOCH": 39,
  "EPOCHS_RUN": 51,
  "CHECKPOINT_PATH": "C:\\Users\\User\\Desktop\\AIN313_PA4\\shapelets_mlp_results\\round_3_mlp_tuning\\20251227_145228_run01_K64_L40-80_cap60_conf1_mlp256x128_lr0.001_ep200_seed42.pt",
  "MACRO_F1": 0.7014895594369938,
  "WEIGHTED_F1": 0.7014895594369938,
  "TOP_CONFUSIONS": "running -> jogging : 6\njogging -> walking : 6\njogging -> running : 6\nhandwaving -> handclapping : 6\nrunning -> walking : 5\nwalking -> running : 4\nhandclapping -> handwaving : 2",
  "PER_CLASS_TABLE": "class            prec    rec     f1   supp\n------------------------------------------\nboxing          1.000  1.000  1.000     20\nhandclapping    0.750  0.900  0.818     20\nhandwaving      0.875  0.700  0.778     20\njogging         0.571  0.400  0.471     20\nrunning         0.474  0.450  0.462     20\nwalking         0.593  0.800  0.681     20",
  "TRAINING_CURVES": "epoch    tr_loss   tr_acc    va_loss   va_acc\n----------------------------------------------\n    1     1.7497   0.2026     1.6073   0.3519\n    2     1.5478   0.4183     1.4558   0.3519\n    3     1.3935   0.4183     1.3174   0.4630\n    4     1.2484   0.4641     1.1963   0.4815\n    5     1.1442   0.5654     1.0968   0.6481\n    6     1.0705   0.5686     1.0165   0.6296\n    7     0.9964   0.5850     0.9535   0.6852\n    8     0.9385   0.6013     0.9046   0.6296\n    9     0.9196   0.5882     0.8329   0.7037\n   10     0.8778   0.6046     0.7677   0.6852\n   11     0.8045   0.6634     0.7310   0.6667\n   12     0.8192   0.6078     0.7054   0.7037\n   13     0.7481   0.6961     0.7111   0.7037\n   14     0.7839   0.6503     0.6763   0.6852\n   15     0.7458   0.6569     0.6547   0.7037\n   16     0.7060   0.7124     0.6293   0.7222\n   17     0.7153   0.6797     0.6517   0.6667\n   18     0.6795   0.7124     0.6461   0.6667\n   19     0.6820   0.7092     0.6181   0.7593\n   20     0.6721   0.6928     0.6059   0.7407\n   21     0.6434   0.7124     0.5871   0.7037\n   22     0.6249   0.7418     0.5868   0.6852\n   23     0.6338   0.7386     0.5790   0.7407\n   24     0.5997   0.7549     0.5762   0.7593\n   25     0.5912   0.7549     0.5655   0.7407\n   26     0.5890   0.7614     0.5648   0.7037\n   27     0.5814   0.7549     0.5569   0.7037\n   28     0.5751   0.7712     0.5618   0.7593\n   29     0.5519   0.7745     0.5522   0.7593\n   30     0.5367   0.7712     0.5460   0.7222\n   31     0.5412   0.7941     0.5476   0.7778\n   32     0.5167   0.7974     0.5536   0.7778\n   33     0.5154   0.8039     0.5345   0.7593\n   34     0.4934   0.8105     0.5330   0.7407\n   35     0.5170   0.8007     0.5395   0.7222\n   36     0.4782   0.8039     0.5384   0.7593\n   37     0.4436   0.8464     0.5274   0.7778\n   38     0.4535   0.8301     0.5193   0.7778\n   39     0.4574   0.8137     0.5235   0.7963\n   40     0.4468   0.8203     0.5213   0.7963\n   41     0.4463   0.8366     0.5285   0.7778\n   42     0.4403   0.8366     0.5185   0.7963\n   43     0.4250   0.8170     0.5118   0.7778\n   44     0.3987   0.8497     0.5117   0.7778\n   45     0.4287   0.8301     0.5226   0.7593\n   46     0.4020   0.8235     0.5208   0.7593\n   47     0.3951   0.8660     0.5175   0.7963\n   48     0.3863   0.8464     0.5223   0.7593\n   49     0.3914   0.8562     0.5231   0.7407\n   50     0.3815   0.8333     0.5081   0.7407\n   51     0.3770   0.8333     0.5125   0.7593"
}
 
[Split / Data]
{
  "total_files": 599,
  "train_files_before_cap": 479,
  "test_files": 120,
  "train_sequences_after_cap": 360,
  "class_names": [
    "boxing",
    "handclapping",
    "handwaving",
    "jogging",
    "running",
    "walking"
  ]
}
 
[Timings (seconds)]
{
  "load_data": 5.6229,
  "sample_shapelets": 0.0062,
  "transform_train": 253.8823,
  "transform_test": 85.7684,
  "mlp_fit": 4.1018,
  "eval": 0.007,
  "total": 349.3896
}
 
[Accuracy]
0.708333

[Confusion Matrix]
            BX    HC    HW    JG    RN    WK
BX     |    20     0     0     0     0     0
HC     |     0    18     2     0     0     0
HW     |     0     6    14     0     0     0
JG     |     0     0     0     8     6     6
RN     |     0     0     0     6     9     5
WK     |     0     0     0     0     4    16
 
[Classification Report]
              precision    recall  f1-score   support

      boxing     1.0000    1.0000    1.0000        20
handclapping     0.7500    0.9000    0.8182        20
  handwaving     0.8750    0.7000    0.7778        20
     jogging     0.5714    0.4000    0.4706        20
     running     0.4737    0.4500    0.4615        20
     walking     0.5926    0.8000    0.6809        20

    accuracy                         0.7083       120
   macro avg     0.7105    0.7083    0.7015       120
weighted avg     0.7105    0.7083    0.7015       120

Macro F1: 0.7014895594369938
Weighted F1: 0.7014895594369938
 
[Top Confusions]
running -> jogging : 6
jogging -> walking : 6
jogging -> running : 6
handwaving -> handclapping : 6
running -> walking : 5
walking -> running : 4
handclapping -> handwaving : 2
 
[Per-class Table]
class            prec    rec     f1   supp
------------------------------------------
boxing          1.000  1.000  1.000     20
handclapping    0.750  0.900  0.818     20
handwaving      0.875  0.700  0.778     20
jogging         0.571  0.400  0.471     20
running         0.474  0.450  0.462     20
walking         0.593  0.800  0.681     20
 
[Training Curves]
epoch    tr_loss   tr_acc    va_loss   va_acc
----------------------------------------------
    1     1.7497   0.2026     1.6073   0.3519
    2     1.5478   0.4183     1.4558   0.3519
    3     1.3935   0.4183     1.3174   0.4630
    4     1.2484   0.4641     1.1963   0.4815
    5     1.1442   0.5654     1.0968   0.6481
    6     1.0705   0.5686     1.0165   0.6296
    7     0.9964   0.5850     0.9535   0.6852
    8     0.9385   0.6013     0.9046   0.6296
    9     0.9196   0.5882     0.8329   0.7037
   10     0.8778   0.6046     0.7677   0.6852
   11     0.8045   0.6634     0.7310   0.6667
   12     0.8192   0.6078     0.7054   0.7037
   13     0.7481   0.6961     0.7111   0.7037
   14     0.7839   0.6503     0.6763   0.6852
   15     0.7458   0.6569     0.6547   0.7037
   16     0.7060   0.7124     0.6293   0.7222
   17     0.7153   0.6797     0.6517   0.6667
   18     0.6795   0.7124     0.6461   0.6667
   19     0.6820   0.7092     0.6181   0.7593
   20     0.6721   0.6928     0.6059   0.7407
   21     0.6434   0.7124     0.5871   0.7037
   22     0.6249   0.7418     0.5868   0.6852
   23     0.6338   0.7386     0.5790   0.7407
   24     0.5997   0.7549     0.5762   0.7593
   25     0.5912   0.7549     0.5655   0.7407
   26     0.5890   0.7614     0.5648   0.7037
   27     0.5814   0.7549     0.5569   0.7037
   28     0.5751   0.7712     0.5618   0.7593
   29     0.5519   0.7745     0.5522   0.7593
   30     0.5367   0.7712     0.5460   0.7222
   31     0.5412   0.7941     0.5476   0.7778
   32     0.5167   0.7974     0.5536   0.7778
   33     0.5154   0.8039     0.5345   0.7593
   34     0.4934   0.8105     0.5330   0.7407
   35     0.5170   0.8007     0.5395   0.7222
   36     0.4782   0.8039     0.5384   0.7593
   37     0.4436   0.8464     0.5274   0.7778
   38     0.4535   0.8301     0.5193   0.7778
   39     0.4574   0.8137     0.5235   0.7963
   40     0.4468   0.8203     0.5213   0.7963
   41     0.4463   0.8366     0.5285   0.7778
   42     0.4403   0.8366     0.5185   0.7963
   43     0.4250   0.8170     0.5118   0.7778
   44     0.3987   0.8497     0.5117   0.7778
   45     0.4287   0.8301     0.5226   0.7593
   46     0.4020   0.8235     0.5208   0.7593
   47     0.3951   0.8660     0.5175   0.7963
   48     0.3863   0.8464     0.5223   0.7593
   49     0.3914   0.8562     0.5231   0.7407
   50     0.3815   0.8333     0.5081   0.7407
   51     0.3770   0.8333     0.5125   0.7593
 
